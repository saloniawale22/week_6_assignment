{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lvfi3cyj6KSV",
        "outputId": "deb2ff4d-d6fd-45a7-d1ec-2685e47ca425"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "import logging\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Files Loading"
      ],
      "metadata": {
        "id": "prgpm-oHATdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_animals(folder_path):\n",
        "    data = {}\n",
        "    doc_id_to_filename = {}\n",
        "    doc_id = 0\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
        "                data[doc_id] = file.read()\n",
        "                doc_id_to_filename[doc_id] = filename\n",
        "                logging.info(f\"Loaded file: {filename} with doc_id: {doc_id}\")\n",
        "                doc_id += 1\n",
        "    return data, doc_id_to_filename"
      ],
      "metadata": {
        "id": "AFovq_bP6Mab"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text-cleaning"
      ],
      "metadata": {
        "id": "yej8oGMJAUaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "  text = re.sub(r\"[^a-zA-Z0-9 ]\", \"\", text)\n",
        "  text = text.lower()\n",
        "  text = \" \".join(text.split())\n",
        "  return text"
      ],
      "metadata": {
        "id": "CkelkVsz6MXI"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "pf47hygAAY14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    return text.lower().split()"
      ],
      "metadata": {
        "id": "wGXnjlzc6MUe"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Term Frequency (TF)"
      ],
      "metadata": {
        "id": "fZTSViLuAdPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def term_frequency(term, document):\n",
        "    return document.count(term) / len(document)"
      ],
      "metadata": {
        "id": "cfC8d1Zi6MR5"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inverse Document Frequency (IDF)"
      ],
      "metadata": {
        "id": "TyWE_5SvAdl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inverse_document_frequency(term, all_documents):\n",
        "    num_docs_containing_term = sum(1 for doc in all_documents if term in doc)\n",
        "    return math.log(len(all_documents) / (1 + num_docs_containing_term))"
      ],
      "metadata": {
        "id": "ePoSo1J-6MPE"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute TF-IDF"
      ],
      "metadata": {
        "id": "PbCXgVrYAlPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tfidf(document, all_documents, vocab):\n",
        "    tfidf_vector = []\n",
        "    for term in vocab:\n",
        "        tf = term_frequency(term, document)\n",
        "        idf = inverse_document_frequency(term, all_documents)\n",
        "        tfidf_vector.append(tf * idf)\n",
        "    return np.array(tfidf_vector)"
      ],
      "metadata": {
        "id": "98XZG5Q06MMb"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cosine Similarities"
      ],
      "metadata": {
        "id": "8naBYE_3Aofg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm_vec1 * norm_vec2) if norm_vec1 * norm_vec2 != 0 else 0"
      ],
      "metadata": {
        "id": "-BEZYhTR6MIB"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Queries Processing\n"
      ],
      "metadata": {
        "id": "4dx1bz1hA0Uk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_queries(query, all_documents, doc_tfidf_vectors, vocab, top_k=4):\n",
        "    tokenized_query = clean_text(query)\n",
        "    query_vector = compute_tfidf (tokenized_query, all_documents, vocab)\n",
        "\n",
        "    similarities = []\n",
        "    for doc_id, doc_vector in enumerate(doc_tfidf_vectors):\n",
        "        similarity = cosine_similarity(query_vector, doc_vector)\n",
        "        similarities.append((doc_id, similarity))\n",
        "\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return similarities[:top_k]"
      ],
      "metadata": {
        "id": "zvg1DszG6MFL"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Function"
      ],
      "metadata": {
        "id": "A7u0KAfpBPJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_docs_ids_to_filenames(doc_id_to_filename):\n",
        "  return [doc_id_to_filename[doc_id] for doc_id in doc_ids]\n",
        "\n",
        "def main():\n",
        "    folder_path = \"/content/drive/MyDrive/Tech400_animals\"\n",
        "\n",
        "    # Load the animals data and document ID to filename mapping\n",
        "    animals_data, doc_id_to_filename = load_animals(folder_path)\n",
        "\n",
        "    # Input query\n",
        "    queries = input(\"Enter the queries: \")\n",
        "\n",
        "    # Tokenize and clean the documents\n",
        "    tokenized_docs = [clean_text(doc) for doc in animals_data.values()]\n",
        "\n",
        "    # Build a vocabulary\n",
        "    vocab = sorted(set(word for doc in tokenized_docs for word in doc))\n",
        "\n",
        "    # Compute TF-IDF vectors for the documents\n",
        "    doc_tfidf_vectors = [compute_tfidf(doc, tokenized_docs, vocab) for doc in tokenized_docs]\n",
        "\n",
        "    # Search the query in the documents\n",
        "    print(f\"Searching results for '{queries}':\")\n",
        "    similarities = process_queries(queries, tokenized_docs, doc_tfidf_vectors, vocab)\n",
        "\n",
        "    # Prepare results\n",
        "    results = [(queries, similarities)]\n",
        "\n",
        "    # If results are found, print the top 4 animals\n",
        "    if similarities:\n",
        "        print(\"\\nTop 4 animals: \")\n",
        "        for idx, (doc_id, score) in enumerate(similarities[:4], 1):\n",
        "            animal_name = os.path.splitext(doc_id_to_filename[doc_id])[0]\n",
        "            print(f\"Animal {idx}: {animal_name}, Score: {score:.4f}\")\n",
        "    else:\n",
        "        print(\"No results found.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbELIQiC8WvU",
        "outputId": "8b8b4f88-4cfa-42ba-fdaa-f7bed6012533"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the queries: wild\n",
            "Searching results for 'wild':\n",
            "\n",
            "Top 4 animals: \n",
            "Animal 1: Dogs, Score: 0.3132\n",
            "Animal 2: Gorillas, Score: 0.3104\n",
            "Animal 3: Bears, Score: 0.3045\n",
            "Animal 4: Chimpanzees, Score: 0.3032\n"
          ]
        }
      ]
    }
  ]
}